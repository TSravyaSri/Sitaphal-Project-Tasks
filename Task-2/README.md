# Task 2: Chat with Website Using RAG Pipeline

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that enables users to interact with both structured and unstructured data extracted from websites. The system crawls, scrapes, and stores website content, converting it into vector embeddings and storing them in a vector database. Users can then query the system for information and receive detailed, context-rich responses generated by a selected Large Language Model (LLM).

## Overview

The goal of this task is to allow users to interact with website content by querying a system that processes the data into embeddings. The system performs the following:
1. Crawling and scraping websites.
2. Converting the extracted content into vector embeddings.
3. Storing these embeddings in a vector database for efficient retrieval.
4. Generating responses to user queries using a pre-trained LLM, leveraging the context-rich data stored in the database.

---

## Functional Requirements

### 1. Data Ingestion
- **Input**: URLs or a list of websites to crawl and scrape.
- **Process**:
  - Crawl and scrape content from target websites.
  - Extract key data fields, metadata, and textual content.
  - Segment content into chunks for better granularity.
  - Convert chunks into vector embeddings using a pre-trained embedding model.
  - Store embeddings in a vector database with associated metadata for efficient retrieval.

### 2. Query Handling
- **Input**: Userâ€™s natural language question.
- **Process**:
  - Convert the user's query into vector embeddings using the same embedding model used for content.
  - Perform a similarity search in the vector database to retrieve the most relevant chunks.
  - Pass the retrieved chunks to the LLM along with a prompt or agentic context to generate a detailed response.

### 3. Response Generation
- **Input**: Relevant information retrieved from the vector database and the user query.
- **Process**:
  - Use the LLM with retrieval-augmented prompts to produce responses with exact values and context.
  - Ensure factuality by incorporating retrieved data directly into the response.

---

## Example Websites

The following websites are used for crawling and scraping content:

- [University of Chicago](https://www.uchicago.edu/)
- [University of Washington](https://www.washington.edu/)
- [Stanford University](https://www.stanford.edu/)
- [University of North Dakota](https://und.edu/)

---

## Setup and Installation

### 1. Clone the Repository
Clone this repository to your local machine:

```bash
git clone <repository_url>
